{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9913f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/stedn/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "import re\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de589e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 148 posts written to '_posts/' and images to 'images/'\n"
     ]
    }
   ],
   "source": [
    "rake = Rake()\n",
    "\n",
    "# Base URL of the site\n",
    "BASE_URL = \"https://flightfree.org\"  # Replace with the actual site\n",
    "START_PAGE_URL = f\"{BASE_URL}\"  # Page with the summary-read-more-link links\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# Directories\n",
    "IMAGE_DIR = \"images\"\n",
    "POSTS_DIR = \"_posts\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(POSTS_DIR, exist_ok=True)\n",
    "\n",
    "# Fetch main listing page\n",
    "response = requests.get(START_PAGE_URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find article links\n",
    "read_more_links = soup.find_all(\"a\", class_=\"summary-read-more-link\")\n",
    "\n",
    "data = []\n",
    "\n",
    "for link in read_more_links:\n",
    "    relative_url = link.get(\"href\")\n",
    "    full_url = urljoin(BASE_URL, relative_url)\n",
    "\n",
    "    article_response = requests.get(full_url, headers=HEADERS)\n",
    "    article_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "    # Title\n",
    "    title_tag = article_soup.find(\"h1\", class_=\"entry-title\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"Untitled\"\n",
    "    title = re.sub(r'[\\\\/*?:\"<>|]', \"\", title)  # Clean title for filenames\n",
    "\n",
    "    # Author\n",
    "    author_tag = article_soup.find(\"span\", class_=\"entry-author\")\n",
    "    author = author_tag.get_text(strip=True) if author_tag else \"Unknown\"\n",
    "\n",
    "    # Date\n",
    "    date_tag = article_soup.find(\"time\", class_=\"dt-published\")\n",
    "    date_str = date_tag[\"datetime\"] if date_tag and \"datetime\" in date_tag.attrs else datetime.now().isoformat()\n",
    "    date_obj = datetime.fromisoformat(date_str)\n",
    "    date_formatted = date_obj.strftime(\"%Y-%m-%d %H:%M:%S +0300\")  # Adjust timezone as needed\n",
    "    date_filename = date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Content\n",
    "    content_div = article_soup.find(\"div\", class_=\"sqs-html-content\")\n",
    "    content = content_div.get_text(strip=True) if content_div else \"\"\n",
    "\n",
    "    description = content[:150] + \"...\" if len(content) > 150 else content\n",
    "    description = re.sub(r'[\\\\/*?:\"<>|]', \"\", description)  # Clean description\n",
    "\n",
    "    # Image\n",
    "    img_tag = article_soup.find_all(\"img\")[1]\n",
    "    img_url = img_tag[\"src\"] if img_tag and \"src\" in img_tag.attrs else \"\"\n",
    "    img_filename = \"\"\n",
    "\n",
    "    # Create slug from title\n",
    "    slug = re.sub(r'\\W+', '-', title.lower()).strip('-')\n",
    "\n",
    "    if img_url:\n",
    "        # Get the image file extension\n",
    "        img_ext = os.path.splitext(urlparse(img_url).path)[1]  # e.g., \".jpg\", \".jpeg\", \".png\"\n",
    "        \n",
    "        # Use the slug (from earlier) to name the image file\n",
    "        img_name = f\"{slug}{img_ext}\"\n",
    "        img_path = os.path.join(IMAGE_DIR, img_name)\n",
    "\n",
    "        try:\n",
    "            img_data = requests.get(img_url, headers=HEADERS).content\n",
    "            with open(img_path, \"wb\") as f:\n",
    "                f.write(img_data)\n",
    "            img_filename = f\"/images/{img_name}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download image {img_url}: {e}\")\n",
    "\n",
    "    rake.extract_keywords_from_text(content)\n",
    "    keywords = rake.get_ranked_phrases_with_scores()\n",
    "    single_word_keywords = [kw for score, kw in keywords if len(kw.strip().split()) < 3]\n",
    "\n",
    "    # Get top N keywords, e.g., top 5\n",
    "    top_keywords = [kw for kw in single_word_keywords[:8]]\n",
    "    # Optionally, turn into slug-style tags\n",
    "    tags = [re.sub(r'\\W+', '-', kw.lower()).strip('-') for kw in top_keywords]\n",
    "\n",
    "    # Prepare markdown content\n",
    "    markdown_content = f\"\"\"---\n",
    "layout: post\n",
    "title:  {title}\n",
    "description: {content[:150]}...\n",
    "date:   {date_formatted}\n",
    "image:  '{img_filename}'\n",
    "tags:   {tags}\n",
    "---\n",
    "{content}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Save markdown file\n",
    "    md_filename = f\"{date_filename}-{slug}.md\"\n",
    "    md_path = os.path.join(POSTS_DIR, md_filename)\n",
    "\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(markdown_content)\n",
    "\n",
    "    # Store metadata in DataFrame\n",
    "    data.append({\n",
    "        \"Title\": title,\n",
    "        \"Author\": author,\n",
    "        \"Date\": date_formatted,\n",
    "        \"Content\": content,\n",
    "        \"Image Path\": img_filename,\n",
    "        \"Article URL\": full_url,\n",
    "        \"Markdown File\": md_filename\n",
    "    })\n",
    "\n",
    "# Create and save DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"articles_data.csv\", index=False)\n",
    "\n",
    "print(f\"✅ {len(df)} posts written to '{POSTS_DIR}/' and images to '{IMAGE_DIR}/'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9303786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
